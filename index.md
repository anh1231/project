# DSC 180A Methodology Assignment 4

### Andy Huang  
ðŸ“§ anh031@ucsd.edu  

**Section:** A06  
**Mentor:** [Hao Zhang]

---

## Brainstorm Prompts

### **1. What is the most interesting topic covered in your domain this quarter?**
The most interesting topic covered in my domain this quarter was simply just coding the architecture for the training of an LLM from scratch. I had no previous experience with LLMs outside of using ChatGPT thus this entire quarter so far has been very enlightening and interesting as whole for me. If I were to point out a specific topic however it would definitely be the topic of MoE implementation. We decided to stick to a standard Llama style dense model due to time constraints for this quarter but I would definitely be interested in learning more about sparse models that implement MoE.

---

### **2. Describe a potential investigation you would like to pursue for your Quarter 2 Project.**
We have not yet trained our main model but once we do I am expecting to run into some bugs and issues in the model being trained/deployed and I want to see if I can maybe do some investigations into optimizing for said bugs or issues or maybe look more into MoE style models as said above.

---

### **3. What is a potential change youâ€™d make to the approach taken in your current Quarter 1 Project?**
The current approach is taken by splitting the section into two groups, system and ml team where the system is responsible for the model's architecture and training specifications while the ml is responsible for the coding of the model itself and the model parameters. Currently we are basically split working on our own respective tasks and only meeting up from time to time for updates on each other's progress, next quarter hopefully we can integrate everything into a single team and thus have a much more efficient team structure that way.

---

### **4. What other techniques would you be interested in using in your project?**
Mixture of Experts (MoE), Finetuning vs Distilling a model, Retrieval Augmented Generation (RAG), Inference Time Scaling techniques, these are all techniques that I would like to research more about to see if it is possible to implement within quarter 2.
---

*Created for DSC 180A - Methodology Assignment 4 (Fall 2025).*
